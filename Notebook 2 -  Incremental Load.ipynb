{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0cdd719-40ec-4300-8f02-93fe1bd1d250",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Incremental Load (New Data Ingestion)\n",
    "\n",
    "The purpose of this second notebook is the ingestion and pre-processing of any newly arriving data. Once the data are properly pre-processed, they are appended to the corresponding table of the PostgreSQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7505cb5-5ef1-4696-b25d-e93480ff90b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### [1] Mount our Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6414a0b-f9ab-4c63-a867-77256e61934c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mount already exists.\n"
     ]
    }
   ],
   "source": [
    "# Checking if mount already exists\n",
    "mnts = dbutils.fs.mounts()\n",
    "mnt_exists = False\n",
    "for mount in mnts:\n",
    "    if mount.mountPoint == \"/mnt/ingestion\":\n",
    "        mnt_exists = True\n",
    "\n",
    "if mnt_exists == False:\n",
    "    # Setup some parameters and keys\n",
    "    account_name = \"trajectoriesstorage\"\n",
    "    container = \"ingestion\"\n",
    "    access_key = dbutils.secrets.get(scope=\"key-vault-connect\", key=\"storage-key\")\n",
    "\n",
    "    # Define the connection configurations\n",
    "    configs = {\n",
    "        \"fs.azure.account.auth.type\": \"key\",\n",
    "        \"fs.azure.account.key.\"+account_name+\".blob.core.windows.net\": access_key\n",
    "    }\n",
    "\n",
    "    # Command to mount the blob storage container locally\n",
    "    dbutils.fs.mount(\n",
    "    source = f\"wasbs://{container}@{account_name}.blob.core.windows.net\",\n",
    "    mount_point = \"/mnt/ingestion\",\n",
    "    extra_configs = configs)\n",
    "else:\n",
    "    print(\"Mount already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34458a28-854f-4e0f-921c-510f359baab1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00c40226-05f7-41a0-b5a3-393d2cae8249",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### [2] Checking file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b599445-e13f-4484-87dc-ad6bfba06fa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a field for a parameter that checks a given filename\n",
    "dbutils.widgets.text(\"file_name\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64088778-31ee-4bb5-bea7-17714148b4de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2430115093446127>:11\u001B[0m\n",
       "\u001B[1;32m      7\u001B[0m path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(directory, filename)\n",
       "\u001B[1;32m      9\u001B[0m goodfile \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n",
       "\u001B[1;32m     12\u001B[0m     num_lines \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m file)\n",
       "\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_lines \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m:\n",
       "\n",
       "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/dbfs/mnt/ingestion/MSG4-202302010000.2-MSG4-202302010015.3.txt'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2430115093446127>:11\u001B[0m\n\u001B[1;32m      7\u001B[0m path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(directory, filename)\n\u001B[1;32m      9\u001B[0m goodfile \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[1;32m     12\u001B[0m     num_lines \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m file)\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_lines \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m:\n\n\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/dbfs/mnt/ingestion/MSG4-202302010000.2-MSG4-202302010015.3.txt'",
       "errorSummary": "<span class='ansi-red-fg'>FileNotFoundError</span>: [Errno 2] No such file or directory: '/dbfs/mnt/ingestion/MSG4-202302010000.2-MSG4-202302010015.3.txt'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = \"/dbfs/mnt/ingestion\"\n",
    "\n",
    "filename = dbutils.widgets.get(\"file_name\")\n",
    "\n",
    "path = os.path.join(directory, filename)\n",
    "\n",
    "goodfile = False\n",
    "    \n",
    "with open(path, 'r') as file:\n",
    "    num_lines = sum(1 for line in file)\n",
    "    if num_lines >= 4:\n",
    "        goodfile = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daecdcca-4178-40b1-98d9-4549da207954",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: True"
     ]
    }
   ],
   "source": [
    "goodfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "385f1296-bf2b-4cc2-8f4e-c7d4d6674cfb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### [3] Loading the contents in a dataframe and uploading them to the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6aaea87-b51f-4b89-9df4-38f07c6e58a1",
     "showTitle": true,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created a dataframe out of the newly appended text file.\nSuccessfully connected to the database.\nAppended the new CloudID into the cloudids table.\nAppended the new data into the dataset table.\n"
     ]
    }
   ],
   "source": [
    "if goodfile == True:\n",
    "\n",
    "    import pandas as pd\n",
    "    import re\n",
    "\n",
    "    import psycopg2\n",
    "    from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "\n",
    "    from pandas.errors import ParserError\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\" \", header=None)\n",
    "        num_columns = len(df.columns)    \n",
    "        if num_columns == 156:\n",
    "            # Include Cloud ID\n",
    "            # Extract the starting timestamp and ID as the CloudID\n",
    "            match = re.search(r\"-([^-]*)-\", filename)\n",
    "            result = match.group(1)\n",
    "            result = result.replace('.', '')\n",
    "            CloudID = int(result)\n",
    "            # Create a CloudID column\n",
    "            rows = len(df) # Count the rows of the file\n",
    "            cloud_id_list = [CloudID]*rows # Create a list of the same number (the cloud ID) with #rows elements\n",
    "            df[\"CloudID\"] = cloud_id_list # Assign a new column in the dataframe with the CloudID\n",
    "\n",
    "            # Rename the year, month, day, hour, minute columns to create a single timestamp column\n",
    "            df.rename(columns={144: \"year\", 145: \"month\", 146: \"day\", 147: \"hour\", 148:\"minute\"}, inplace=True)\n",
    "\n",
    "            # More renames\n",
    "            df.rename(columns={0: \"ID\", 2 : \"Lat\", 3 : \"Long\", 149 : \"Direction\", 1 : \"Area_Size\", 9 : \"Axis\"}, inplace=True)\n",
    "\n",
    "            # Add the timestamp column\n",
    "            df[\"Timestamp\"] = pd.to_datetime(df[['year', 'month', 'day', 'hour', 'minute']])\n",
    "\n",
    "            # Drop the column with the typos\n",
    "            df.drop([28], axis=1, inplace=True)\n",
    "\n",
    "            # Replace the symbols ## with 0 for initial rate of change value\n",
    "            df.replace(\"##\",0.0)\n",
    "\n",
    "            final_df = df[['ID', 'Area_Size', 'Lat', 'Long', 'Axis', 'Direction', 'CloudID', 'Timestamp']].copy()\n",
    "\n",
    "            print(\"Successfully created a dataframe out of the newly appended text file.\")\n",
    "\n",
    "            # Set parameters for initial connection to new-built database server\n",
    "            host = 'postgresbase-trajectories-server.postgres.database.azure.com'\n",
    "            database = 'clouddb'\n",
    "            user = 'cloudadmin'\n",
    "            password = dbutils.secrets.get(scope=\"key-vault-connect\", key=\"postgres-password\")\n",
    "            sslmode = 'require'\n",
    "\n",
    "            # Connect to the PostgreSQL server\n",
    "            conn_string = f\"host={host} user={user} dbname={database} password={password} sslmode={sslmode}\"\n",
    "            conn = psycopg2.connect(conn_string)\n",
    "\n",
    "            # We have to add this here\n",
    "            conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "\n",
    "            # Open a cursor to perform database operations\n",
    "            cur = conn.cursor()\n",
    "\n",
    "            print(\"Successfully connected to the database.\")\n",
    "\n",
    "            # query to insert the new filename-CloudID correspondence\n",
    "            query_to_insert = f\"INSERT INTO cloudids (filenames, cloudid) VALUES ('{filename}', {CloudID});\"\n",
    "\n",
    "            # execute the query\n",
    "            cur.execute(query_to_insert)\n",
    "\n",
    "            print(\"Appended the new CloudID into the cloudids table.\")\n",
    "\n",
    "            # Make the list containing tuples (...)\n",
    "            dfdata = []\n",
    "\n",
    "            for index, row in final_df.iterrows():\n",
    "                dfdata.append((row['ID'], row['Area_Size'], row['Lat'], row['Long'], row['Axis'], row['Direction'], row['CloudID'], row['Timestamp']))\n",
    "                \n",
    "            # query to insert\n",
    "            query_to_insert = \"INSERT INTO dataset (id, area_size, lat, long, axis, direction, cloudid, timestamp) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "\n",
    "            # execute the query\n",
    "            cur.executemany(query_to_insert, dfdata)\n",
    "\n",
    "            # Close the cursor and database connection\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            print(\"Appended the new data into the dataset table.\")\n",
    "        else:\n",
    "            print(\"The file is bad: detected more/less than 156 columns.\")\n",
    "\n",
    "    except ParserError:\n",
    "        print(\"The file is bad: some lines have a different number of columns than others.\")\n",
    "\n",
    "else:\n",
    "    print(\"The file is bad: it contains less than 4 lines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32d79383-5c10-4e75-aa39-ce8c1b13b62a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34670c55-417a-4fb3-b343-b35d635f9a70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b8c417f-5947-4e7a-a717-17b18e77c293",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bcbe0d1-50f8-4fea-8704-6d318cdeaee9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d23e5ddf-2bbc-4cff-9d2d-ebb1a21abaf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fcf5546-a9c6-48f8-ba00-2549101f0c1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "816cdb09-f6d9-4d84-939b-4ff98b0ca792",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3320ae8-f42f-463e-9da0-27f840f08a15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook 2 -  Incremental Load",
   "widgets": {
    "file_name": {
     "currentValue": "MSG4-202302010000.2-MSG4-202302010015.3.txt",
     "nuid": "c4a54e42-2c86-4a0e-a15f-4f3022bf2b84",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "file_name",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
